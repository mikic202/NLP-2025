{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import kagglehub\n",
    "import torch\n",
    "import nltk\n",
    "from utils.text_datasets import get_basic_tweet_sentiment_dataset, get_poem_sentiment_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_statistics(dataset, remove_stopwords=False, top_n_words=10):\n",
    "    num_samples = len(dataset)\n",
    "    sample_lengths = [len(sample) for sample, _ in dataset]\n",
    "    avg_length = np.mean(sample_lengths)\n",
    "\n",
    "    words_in_set = [word for sample, _ in dataset for word in sample.split()]\n",
    "\n",
    "    if remove_stopwords:\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "        words_in_set = [word for word in words_in_set if word not in stopwords]\n",
    "\n",
    "    word_counts = pd.Series(words_in_set).value_counts()\n",
    "\n",
    "    return {\n",
    "        'num_samples': num_samples,\n",
    "        'avg_length': avg_length,\n",
    "        'num_words': len(words_in_set),\n",
    "        'avg_words': len(words_in_set) / num_samples,\n",
    "        'num_unique_words': len(word_counts),\n",
    "        'unique_words': word_counts[:top_n_words],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Poem Sentiment dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since google-research-datasets/poem_sentiment couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at /home/mikic202/.cache/huggingface/datasets/google-research-datasets___poem_sentiment/default/0.0.0/685b95a2787a869b7bae6c4480810f57fe23b48e (last modified on Tue Apr 22 21:39:59 2025).\n"
     ]
    }
   ],
   "source": [
    "whole_poem_dataset = torch.utils.data.ConcatDataset(get_poem_sentiment_dataset(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_samples': 1101,\n",
       " 'avg_length': 38.328792007266124,\n",
       " 'num_words': 4570,\n",
       " 'avg_words': 4.150772025431426,\n",
       " 'num_unique_words': 3190,\n",
       " 'unique_words': thy       30\n",
       " like      23\n",
       " shall     19\n",
       " would     18\n",
       " thou      17\n",
       " yet       16\n",
       " see       16\n",
       " upon      13\n",
       " though    13\n",
       " three     12\n",
       " Name: count, dtype: int64}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_text_statistics(whole_poem_dataset, remove_stopwords=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Twitter Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mikic202/miniconda3/envs/pt/lib/python3.11/site-packages/datasets/load.py:1491: FutureWarning: The repository for stanfordnlp/sentiment140 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/stanfordnlp/sentiment140\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "whole_basic_tweet_dataset = torch.utils.data.ConcatDataset(get_basic_tweet_sentiment_dataset(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_samples': 1600498,\n",
       " 'avg_length': 74.09207821565538,\n",
       " 'num_words': 14047412,\n",
       " 'avg_words': 8.776900689660343,\n",
       " 'num_unique_words': 1350958,\n",
       " 'unique_words': I        496739\n",
       " I'm       99579\n",
       " get       76748\n",
       " like      73315\n",
       " -         67121\n",
       " go        62987\n",
       " good      59797\n",
       " day       55756\n",
       " got       53890\n",
       " going     53248\n",
       " Name: count, dtype: int64}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_text_statistics(whole_basic_tweet_dataset, remove_stopwords=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
