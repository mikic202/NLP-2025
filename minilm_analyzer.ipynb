{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from utils.text_datasets import get_basic_tweet_sentiment_dataset, get_poem_sentiment_dataset, get_advanced_tweet_sentiment_dataset_only_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "MODEL_NAME = 'all-MiniLM-L6-v2'\n",
    "TEST_SIZE = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minilm_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def tokenize_function_minilm(text):\n",
    "    return minilm_model.encode(text, convert_to_numpy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset, test_dataset = get_poem_sentiment_dataset(tokenize_function_minilm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [x.numpy() for x, y in train_dataset]\n",
    "y_train = [y.item() for x, y in train_dataset]\n",
    "\n",
    "X_val = [x.numpy() for x, y in val_dataset]\n",
    "y_val = [y.item() for x, y in val_dataset]\n",
    "\n",
    "X_test = [x.numpy() for x, y in test_dataset]\n",
    "y_test = [y.item() for x, y in test_dataset]\n",
    "\n",
    "# Trenujemy klasyfikator\n",
    "clf = GradientBoostingClassifier()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# 9. Macierz pomyłek\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel(\"Predykcja\")\n",
    "plt.ylabel(\"Prawdziwa etykieta\")\n",
    "plt.title(\"Macierz pomyłek\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "\n",
    "with open(\"config_transformers.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        key, value = line.strip().split(\"=\")\n",
    "        try:\n",
    "            value = int(value)\n",
    "        except ValueError:\n",
    "            try:\n",
    "                value = float(value)\n",
    "            except ValueError:\n",
    "                pass\n",
    "        params[key] = value\n",
    "\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(params[\"model_bert\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function_bert(text):\n",
    "    encoding = tokenizer(\n",
    "        text,\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=params[\"max_length_token\"],\n",
    "        return_tensors=None\n",
    "    )\n",
    "    return torch.tensor(encoding[\"input_ids\"])\n",
    "\n",
    "\n",
    "def pad_collate_bert(batch):\n",
    "    input_ids, labels = zip(*batch)\n",
    "    attention_masks = []\n",
    "\n",
    "    padded_input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    for ids in input_ids:\n",
    "        mask = torch.ones(len(ids), dtype=torch.long)\n",
    "        attention_masks.append(mask)\n",
    "    padded_attention_mask = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": padded_input_ids,\n",
    "        \"attention_mask\": padded_attention_mask,\n",
    "        \"labels\": torch.tensor(labels)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, validation_dataset, test_dataset = get_poem_sentiment_dataset(tokenize_function_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = get_basic_tweet_sentiment_dataset(tokenize_function_bert)\n",
    "train_dataset, validation_dataset = train_test_split(\n",
    "    train_dataset, train_size=params[\"train_size\"], shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = get_advanced_tweet_sentiment_dataset_only_text(tokenize_function_bert)\n",
    "train_dataset, validation_dataset = train_test_split(\n",
    "    train_dataset, train_size=params[\"train_size\"], shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_num = params[\"class_num\"]\n",
    "dataset_labels = list(range(class_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=params[\"batch_size\"], shuffle=True, collate_fn=pad_collate_bert)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=params[\"batch_size\"], shuffle=False, collate_fn=pad_collate_bert)\n",
    "test_loader = DataLoader(test_dataset, batch_size=params[\"batch_size\"], shuffle=False, collate_fn=pad_collate_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(params[\"model_bert\"], num_labels=params[\"class_num\"]).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=params[\"learning_rate\"])\n",
    "loss_fun = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "progress_bar = tqdm.tqdm(range(params[\"epochs\"]), desc=\"Epoch\")\n",
    "\n",
    "for epoch in progress_bar:\n",
    "    total_loss = 0\n",
    "    batches = 0\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        batches += 1\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        val_batches = 0\n",
    "        for batch in validation_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            val_loss += outputs.loss.item()\n",
    "            val_batches += 1\n",
    "\n",
    "    model.train()\n",
    "    progress_bar.set_postfix({\n",
    "        \"Train loss\": total_loss / batches,\n",
    "        \"Validation loss\": val_loss / val_batches\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    return all_labels, all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train set metrics\")\n",
    "train_labels, train_preds = measure_model(model, train_loader)\n",
    "train_preds_tensor = torch.tensor(train_preds)\n",
    "train_labels_tensor = torch.tensor(train_labels)\n",
    "\n",
    "display_clasification_metrics(train_preds_tensor, train_labels_tensor, labels=dataset_labels)\n",
    "\n",
    "\n",
    "print(\"\\nTest set metrics\")\n",
    "test_labels, test_preds = measure_model(model, test_loader)\n",
    "test_preds_tensor = torch.tensor(test_preds)\n",
    "test_labels_tensor = torch.tensor(test_labels)\n",
    "\n",
    "display_clasification_metrics(test_preds_tensor, test_labels_tensor, labels=dataset_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
